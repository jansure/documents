参考博客：http://www.imooc.com/article/270570

可以在网页查看各个服务运行状态 ： 
    HDFS  :  http://114.115.128.138:50070    
    ResourceManager :  http://114.115.128.138:8088

#hadoop安装目录： 
    软件位置：  /usr/local/soft/hadoop
    配置文件目录： /usr/local/soft/hadoop/etc/hadoop
    datanode节点位置：  /data/hadoop/tmp

ssh-copy-id -i ~/.ssh/id_rsa.pub s2
ssh-copy-id -i ~/.ssh/id_rsa.pub s3
ssh-copy-id -i ~/.ssh/id_rsa.pub s4


hadoop-env.sh
    export JAVA_HOME=/home/hadoop/app/jdk1.7.0_79


core-site.xml
    <property>
        <name>fs.default.name</name>
        <value>hdfs://s2:8020</value>
    </property>

hdfs-site.xml

   F:\e-develop\2019-code\vscode-2019\linux-study\TXT\hadoop集群

*******服务器******
     <property>
        <name>dfs.namenode.name.dir</name>
        <value>/data/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/data/hadoop/tmp/dfs/data</value>
    </property>


yarn-site.xml

    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
   <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>s2</value>
   </property>

mapred-site.xml

    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
修改：
    slaves
        s1
        s2
        s3


2）分发安装包到hadoop001和hadoop002节点上
（注意：三台机器的用户名都是一样的，只是机器名不一样哦）

#分发hadoop和jdk，因文件比较大、分发过程可能有点慢scp -r ~/app hadoop@hadoop001:~/scp -r ~/app hadoop@hadoop002:~/
    #分发配置文件scp ~/.bash_profile hadoop@hadoop001:~/scp ~/.bash_profile hadoop@hadoop002:~/
在学习过程中强烈推荐使用伪分布式
在hadoop001和hadoop002机器上让.bash_profile生效


 tar -xf hadoop-native-64-2.6.0.tar -C hadoop-cdh/lib/native/


#分别在hadoop001和hadoop002中执行source ~/.bash_profile
3）对NameNode做格式化：只要在hadoop000机器上执行即可

hdfs namenode -format
4）启动集群:只要在hadoop000上执行即可

sbin/start-all.sh
5）验证：jps




6）停止机器：在hadoop000上执行即可

sbin/stop-all.sh
至此，hadoop集群搭建完毕！



#问题解决
1、WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
现在native（下载2.6对应的tar包）：http://dl.bintray.com/sequenceiq/sequenceiq-bin/
配置native环境变量：https://blog.csdn.net/u010003835/article/details/81127984


    export  HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export  HADOOP_HOME=/usr/local/soft/hadoop
    export  HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_COMMON_LIB_NATIVE_DIR"

 hdfs数据目录存储位置: https://blog.csdn.net/opensure/article/details/51452058   
      hadoop fs -cat  /data/hadoop/tmp/dfs/data/current/BP-832409145-192.168.1.136-1552534912651/current/



 #解决方案：
解决方案：namenode is  safe mode; http://www.cnblogs.com/taisenki/p/4643710.html     
<property>
  <name>dfs.namenode.replication.min</name>
  <value>1</value>
  <description>Minimal block replication. 
  </description>
</property>

<property>
  <name>dfs.namenode.safemode.threshold-pct</name>
  <value>0.999f</value>
  <description>
    Specifies the percentage of blocks that should satisfy the minimal
    replication requirement defined by dfs.namenode.replication.min.
    Values less than or equal to 0 mean not to wait for any particular
    percentage of blocks before exiting safemode.
    Values greater than 1 will make safe mode permanent.
  </description>
</property>
复制代码


##参考链接： https://www.cnblogs.com/alex-blog/p/3179939.html
增加机器不重启操作如下： 
首先，把新节点的 IP或主机名 加入主节点（master）的 conf/slaves 文件。 
然后登录新的从节点，执行以下命令： 
$ cd path/to/hadoop 
$ bin/hadoop-daemon.sh start datanode 
$ bin/hadoop-daemon.sh start tasktracker 
然后就可以在namanode机器上运行balancer，执行负载均衡 
$bin/hadoop  balancer